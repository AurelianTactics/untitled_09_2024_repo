{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "baseline version that is basically without using a graph\n",
    "see how much can be pushed to the LLM\n",
    "\n",
    "working\n",
    "prompt\n",
    "    use some of your old prompts and questions\n",
    "\n",
    "to do\n",
    "prompt\n",
    "simple graph\n",
    "gradio interface\n",
    "    look over old notes\n",
    "evaluate\n",
    "    better testing questions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "backlog\n",
    "    see other notes for that\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from langchain_openai import OpenAI, ChatOpenAI\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import Runnable, RunnableConfig\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from typing import Annotated, Literal, Optional\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "import pgeocode\n",
    "\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "_set_env(\"LANGSMITH_API_KEY\")\n",
    "# _set_env(\"TAVILY_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"EV Prototype v0.4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_questions_list = [\n",
    "    'What is the cost of owning a 2024 new Hyundai Ioniq versus a 2024 Hyundai Venue?',\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_graph(graph):\n",
    "    try:\n",
    "        display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "    except Exception:\n",
    "        # This requires some extra dependencies and is optional\n",
    "        pass\n",
    "\n",
    "\n",
    "def _print_event(event: dict, _printed: set, max_length=1500):\n",
    "    current_state = event.get(\"dialog_state\")\n",
    "    if current_state:\n",
    "        print(\"Currently in: \", current_state[-1])\n",
    "    message = event.get(\"messages\")\n",
    "    if message:\n",
    "        if isinstance(message, list):\n",
    "            message = message[-1]\n",
    "        if message.id not in _printed:\n",
    "            msg_repr = message.pretty_repr(html=True)\n",
    "            if len(msg_repr) > max_length:\n",
    "                msg_repr = msg_repr[:max_length] + \" ... (truncated)\"\n",
    "            print(msg_repr)\n",
    "            _printed.add(message.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v 0.01 Minimal LLM based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "    user_info: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Assistant:\n",
    "    def __init__(self, runnable: Runnable):\n",
    "        self.runnable = runnable\n",
    "\n",
    "    def __call__(self, state: State, config: RunnableConfig):\n",
    "        while True:\n",
    "            result = self.runnable.invoke(state)\n",
    "\n",
    "            if not result.tool_calls and (\n",
    "                not result.content\n",
    "                or isinstance(result.content, list)\n",
    "                and not result.content[0].get(\"text\")\n",
    "            ):\n",
    "                messages = state[\"messages\"] + [(\"user\", \"Respond with a real output.\")]\n",
    "                state = {**state, \"messages\": messages}\n",
    "            else:\n",
    "                break\n",
    "        return {\"messages\": result}\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "\n",
    "ev_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an assistant that helps gather information for people deciding to buy or lease an electric vehicle or a gas car.\"\n",
    "            \"Help them compare the costs of buying or leasing an electric vehicle or a gas car.\"\n",
    "            \"The fundamental question you are trying to answer is whether it is cheaper to buy an electric vehicle or a gas car.\"\n",
    "            \"If the user does not provide detailed information, answer the question generally.\"\n",
    "            \"If the user does provide detailed information, use that information to provider a better answer.\"\n",
    "            \"Try to ask the user for more information after you provider your answer.\"\n",
    "            \"Some examples of key pieces of information to ask include:\\n\"\n",
    "            \" - Location\\n\"\n",
    "            \" - Used or new\\n\"\n",
    "            \" - Type of car they are interested in (sedan, luxury, compact-SUV, mid-size truck, etc.) \\n\"\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "ev_assistant_runnable = ev_prompt | llm\n",
    "\n",
    "builder = StateGraph(State)\n",
    "\n",
    "builder.add_node(\"assistant\", Assistant(ev_assistant_runnable))\n",
    "# builder.set_entry_point(\"assistant\")\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_edge(\"assistant\", END)\n",
    "\n",
    "memory = MemorySaver()\n",
    "baseline_agent_graph = builder.compile(\n",
    "    checkpoint=memory,\n",
    ")\n",
    "\n",
    "display_graph(baseline_agent_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_id = str(uuid.uuid4())\n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "\n",
    "        # Checkpoints are accessed by thread_id\n",
    "        \"thread_id\": thread_id,\n",
    "    }\n",
    "}\n",
    "\n",
    "_printed = set()\n",
    "for question in testing_questions_list:\n",
    "    events = baseline_agent_graph.stream(\n",
    "        {\"messages\": (\"user\", question)}, config, stream_mode=\"values\"\n",
    "    )\n",
    "    for event in events:\n",
    "        _print_event(event, _printed)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sep2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
