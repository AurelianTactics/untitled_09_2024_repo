{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nworking\\n    \\n    \\n    need a tool to parse the info known so far from the user about vehicle 1 and vehicle 2\\n        then this is passed into the ask tools to get the costs\\n        this info is potentially updated everytime the user does an input\\n        backlog / maybe this way: additional user output instructions\\n\\n    parse input tool needs some defualts if not parsed\\n        maybe in description or in the prompt\\n        maybe examples\\n\\n    try the parse tool\\n        and debug\\n\\n    add something that user should only be asked about car related stuff\\n\\n    hook up the tools\\n        need the output from the try and parse tool\\n        maybe the docs string wiht a bunch of kwargs? or maybe a custom class?\\n\\n    try the tools\\n\\n\\n    see how to hook up the evaluation\\n        DONE read through docs/tutorial\\n\\n        want the auto evaluation with specific examples to work too\\n\\n        rough plan:\\n            make a dataset\\t\\t\\t\\n            step through and do the human eval\\t\\t\\t\\n                see how feasible this is for checking things\\t\\t\\n            do one heuristic and backlog the rest\\t\\t\\t\\n                single step (ie tool selection)\\t\\t\\n                single anser (ie tool get right answer)\\t\\t\\n                trajectory (ie go through the correct process\\t\\t\\n            do one LLM as a judge\\t\\t\\t\\n                        \\n            for each of the above, use a tutorial. just want osmething smoke tested\\t\\t\\n\\n            \\n    provide examples\\n        overall question\\n        fuel tools\\n        maintenance tools\\t\\n            \\n        \\n    DONE stopped at parse input user tools description and prompt\\n\\n    DONE update the entry prompt\\n    \\n    DONE read the joiner prompt\\n    DONE read more about the joiner\\n        try to answer you questions\\n\\n    TEST joiner prompt\\n        DONE find where the initial prompt example is written to so I can do something similar\\n            https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html\\n            https://smith.langchain.com/hub/wfh/llm-compiler-joiner\\n        DONE need a new base prompt\\n            want format of like: intiial cost vheicle 1 and initial cost vehicle two\\n                cost over x  years for vehicle 1, and for vehicle two\\n            then have cost by year\\n                just the yearly summary side by side\\n                    year, vehicle 1, vehicle 2\\n                    summed at the bottom\\n            then have the cost by item\\n                item, vehicle 1, vehicle 2\\n\\n            then if they want the item broken down by year, user can request taht\\n\\n            try to leave it for the LLM\\n                make tools if needed\\n\\n        need the messages placeholder\\n        want the examples placeholder as well\\n\\n    DONE tavily research\\n        DONE try some include domain examples\\n        find some example domains for the federal tax credit\\n        maybe some example domains for state tax credit\\n        see how tavily works with shopping\\n            thinking just try some search results and see how it goes\\n                may be include certain domains\\n            this might be something to ask the LLM\\n\\n    DONE research car price tools\\n        research the 3 options and see what might work\\n            tavily web search\\n            API\\n            ask the llm\\n        minimal version should be ask the LLM, pretty good for a starting point\\n            user has to find a specific deal to get more exact\\n\\n\\n    MEH tavily tools\\n        federal tax credit\\n            RAG it? context it?\\n            just let it search?\\n            serach with a specific base website (like irs)\\n        state tax credit\\n    \\n    TEST find car price tools\\n        ask the LLM\\n\\n    TEST used car tool for federal credit\\n        RAG then ask the LLM\\n        can grab a specific website\\n    \\n    TEST new car tool for federal credit\\n        RAG then ask the LLM\\n        may or may not be able to ask the specific website\\n            requires a weird drop down. may have to cache part of the results\\n\\n\\n    TEST work on the output final look\\n        as this will influence how the tools work\\n        have it mocked up but how do I connect it to the below pipeline?\\n        research and test some output examples. should come out of hte box for LLM\\n        feed in some examples\\n\\n    TEST create tools for my simple value version\\n        cars available\\n            use google or main website? how does tavily do for shopping\\n        federal tax credit info\\n            can tavily search a specific website\\n        TEST average maintenance costs\\n        TEST fuel and gas costs\\n        DONE LLM turns a monthly cost into a yearly cost\\n            adds the yearly costs\\n            can use existing math tools\\n            have to find a way to format the question correctly\\n            then get the output in a format that works\\n                ask the LLM or give a specific structured output?\\n        TRY WITH LLM FIRST formats the outputs correctly\\n\\n\\n    DONE get some version of it working\\n        STOPPED AT PLANNER\\n        may have to revert back to full tutorial code first to see\\n    DONE read through the tutorial\\n        rough outline\\n    DONE read through the paper\\n    DONE see the other tutorial examples in experimental\\n    \\n\\nto do\\n    evaluations\\n\\n    user is able to input or goes to the tools\\n    \\n    limit what hte user can ask\\n        for now just cost of the vehicle\\n\\n    focus on buy only\\n        simple things first, make notes for where more improvements\\n        new / used\\n            by model and num miles\\n        federal tax credit only\\n        costs\\n            maintenance\\n                does this depend on miles driven per yer? startw ith default, let user change\\n            insurance\\n            registration\\n            fuel\\n\\n    after initial question, ask for more details with an example\\n        new/used (and how many miles)\\n        type of car\\n        zip code / location\\n        mileage driven per year (fuel costs, maintenance costs)\\n\\n\\n    DONE have to pass in the tool description I think\\n    \\n    DONE review the output parser. may be able to repurpose some of these\\n        https://langchain-ai.github.io/langgraph/tutorials/llm-compiler/LLMCompiler/#output-parser\\n    DONE review the example tools. may be able to repurpose some of these\\n        https://langchain-ai.github.io/langgraph/tutorials/llm-compiler/LLMCompiler/#math-tools\\n\\n    write the tools\\n    organize tools\\n\\n    incorporate old prototype notes\\n    DONE incorporate other notes file\\n\\n    read through the github code\\n\\n    example prompts in the input or on the interface\\n\\n    langsmith logging\\n    \\n    evaluation suite\\n        what is factually correct like the federal tax prompts\\n\\n    examples for hte prompts\\n\\n    consolidate all my backlog and notes in one place when done\\n\\nconcerns\\n    lot of new stuff here for me to digest\\n    will be much harder to get the minimal version up. maybe work on the smaller version simple one in parts at times too\\n\\n    not really sure about the maintenance and fuel tools\\n        how to hook up\\n        how they work\\n        if the output being passed back is structured enough\\n\\n    think my tools are not really correctly done\\n\\n    not sure if I need a tool for used/new distinction or if the LLM can figure it out on toola ssignment\\n\\nquestions\\n    not getting what the messages placeholder is filled in\\n    need to do a lot more research on other things that are available\\n\\nReferences\\n# https://langchain-ai.github.io/langgraph/tutorials/llm-compiler/LLMCompiler/\\n# https://arxiv.org/abs/2312.04511\\n# https://github.com/SqueezeAILab/LLMCompiler\\n\\nbacklog\\n    additional user output instructions\\n\\n    user feedback for correct or incorrect info\\n        for verifying rebate / maintenance info\\n\\n    using like ab ackground process to get info for certain things rather than query from scratch every time\\n        lay out how things would be done long term if storing some results\\n            background process for updates to federal data\\n            background process for updates to state or local data, then cache and store in DB\\n                or the first time it is done then store it\\n            maybe for insurance info too\\n\\n    output\\n        tools for output if not being formated better\\n        examples for output if not being formatted better\\n\\n    more general EV questiosn\\n\\n    understand the chatprompttemplate better\\n        placeholders\\n        hub\\n        how to better use existing prompts, write and contritube prompts\\n\\n    async implementation of tavily needed?\\n\\n    tavily alternatives\\n\\n    tavily with get context or more base method than the built in tool\\n\\n    better new and used car price tool\\n\\n    new and used federal tax credit tools\\n        make context dynamic\\n        way to make the questions more dynamic\\n        is this even the best way to do it?\\n\\n    can probably store a lot of the data and answers\\n    non US locations\\n    can specifically ask for\\n        kw/hour cost\\n        per gallon gas price\\n    more advanced reasoning graphs\\n    Cost of capital and more advanced financial analysis\\n\\n    advanced RAG of key information\\n        ie like the IRS website and how often should I updated it\\n\\n    tools probably not needed anymore\\n        zip to state: LLM seems able to handle this\\n\\n    handle up to x vehicles\\n    \\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "working\n",
    "    \n",
    "    \n",
    "    need a tool to parse the info known so far from the user about vehicle 1 and vehicle 2\n",
    "        then this is passed into the ask tools to get the costs\n",
    "        this info is potentially updated everytime the user does an input\n",
    "        backlog / maybe this way: additional user output instructions\n",
    "\n",
    "    parse input tool needs some defualts if not parsed\n",
    "        maybe in description or in the prompt\n",
    "        maybe examples\n",
    "\n",
    "    try the parse tool\n",
    "        and debug\n",
    "\n",
    "    add something that user should only be asked about car related stuff\n",
    "\n",
    "    hook up the tools\n",
    "        need the output from the try and parse tool\n",
    "        maybe the docs string wiht a bunch of kwargs? or maybe a custom class?\n",
    "\n",
    "    try the tools\n",
    "\n",
    "\n",
    "    see how to hook up the evaluation\n",
    "        DONE read through docs/tutorial\n",
    "\n",
    "        want the auto evaluation with specific examples to work too\n",
    "\n",
    "        rough plan:\n",
    "            make a dataset\t\t\t\n",
    "            step through and do the human eval\t\t\t\n",
    "                see how feasible this is for checking things\t\t\n",
    "            do one heuristic and backlog the rest\t\t\t\n",
    "                single step (ie tool selection)\t\t\n",
    "                single anser (ie tool get right answer)\t\t\n",
    "                trajectory (ie go through the correct process\t\t\n",
    "            do one LLM as a judge\t\t\t\n",
    "                        \n",
    "            for each of the above, use a tutorial. just want osmething smoke tested\t\t\n",
    "\n",
    "            \n",
    "    provide examples\n",
    "        overall question\n",
    "        fuel tools\n",
    "        maintenance tools\t\n",
    "            \n",
    "        \n",
    "    DONE stopped at parse input user tools description and prompt\n",
    "\n",
    "    DONE update the entry prompt\n",
    "    \n",
    "    DONE read the joiner prompt\n",
    "    DONE read more about the joiner\n",
    "        try to answer you questions\n",
    "\n",
    "    TEST joiner prompt\n",
    "        DONE find where the initial prompt example is written to so I can do something similar\n",
    "            https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html\n",
    "            https://smith.langchain.com/hub/wfh/llm-compiler-joiner\n",
    "        DONE need a new base prompt\n",
    "            want format of like: intiial cost vheicle 1 and initial cost vehicle two\n",
    "                cost over x  years for vehicle 1, and for vehicle two\n",
    "            then have cost by year\n",
    "                just the yearly summary side by side\n",
    "                    year, vehicle 1, vehicle 2\n",
    "                    summed at the bottom\n",
    "            then have the cost by item\n",
    "                item, vehicle 1, vehicle 2\n",
    "\n",
    "            then if they want the item broken down by year, user can request taht\n",
    "\n",
    "            try to leave it for the LLM\n",
    "                make tools if needed\n",
    "\n",
    "        need the messages placeholder\n",
    "        want the examples placeholder as well\n",
    "\n",
    "    DONE tavily research\n",
    "        DONE try some include domain examples\n",
    "        find some example domains for the federal tax credit\n",
    "        maybe some example domains for state tax credit\n",
    "        see how tavily works with shopping\n",
    "            thinking just try some search results and see how it goes\n",
    "                may be include certain domains\n",
    "            this might be something to ask the LLM\n",
    "\n",
    "    DONE research car price tools\n",
    "        research the 3 options and see what might work\n",
    "            tavily web search\n",
    "            API\n",
    "            ask the llm\n",
    "        minimal version should be ask the LLM, pretty good for a starting point\n",
    "            user has to find a specific deal to get more exact\n",
    "\n",
    "\n",
    "    MEH tavily tools\n",
    "        federal tax credit\n",
    "            RAG it? context it?\n",
    "            just let it search?\n",
    "            serach with a specific base website (like irs)\n",
    "        state tax credit\n",
    "    \n",
    "    TEST find car price tools\n",
    "        ask the LLM\n",
    "\n",
    "    TEST used car tool for federal credit\n",
    "        RAG then ask the LLM\n",
    "        can grab a specific website\n",
    "    \n",
    "    TEST new car tool for federal credit\n",
    "        RAG then ask the LLM\n",
    "        may or may not be able to ask the specific website\n",
    "            requires a weird drop down. may have to cache part of the results\n",
    "\n",
    "\n",
    "    TEST work on the output final look\n",
    "        as this will influence how the tools work\n",
    "        have it mocked up but how do I connect it to the below pipeline?\n",
    "        research and test some output examples. should come out of hte box for LLM\n",
    "        feed in some examples\n",
    "\n",
    "    TEST create tools for my simple value version\n",
    "        cars available\n",
    "            use google or main website? how does tavily do for shopping\n",
    "        federal tax credit info\n",
    "            can tavily search a specific website\n",
    "        TEST average maintenance costs\n",
    "        TEST fuel and gas costs\n",
    "        DONE LLM turns a monthly cost into a yearly cost\n",
    "            adds the yearly costs\n",
    "            can use existing math tools\n",
    "            have to find a way to format the question correctly\n",
    "            then get the output in a format that works\n",
    "                ask the LLM or give a specific structured output?\n",
    "        TRY WITH LLM FIRST formats the outputs correctly\n",
    "\n",
    "\n",
    "    DONE get some version of it working\n",
    "        STOPPED AT PLANNER\n",
    "        may have to revert back to full tutorial code first to see\n",
    "    DONE read through the tutorial\n",
    "        rough outline\n",
    "    DONE read through the paper\n",
    "    DONE see the other tutorial examples in experimental\n",
    "    \n",
    "\n",
    "to do\n",
    "    evaluations\n",
    "\n",
    "    user is able to input or goes to the tools\n",
    "    \n",
    "    limit what hte user can ask\n",
    "        for now just cost of the vehicle\n",
    "\n",
    "    focus on buy only\n",
    "        simple things first, make notes for where more improvements\n",
    "        new / used\n",
    "            by model and num miles\n",
    "        federal tax credit only\n",
    "        costs\n",
    "            maintenance\n",
    "                does this depend on miles driven per yer? startw ith default, let user change\n",
    "            insurance\n",
    "            registration\n",
    "            fuel\n",
    "\n",
    "    after initial question, ask for more details with an example\n",
    "        new/used (and how many miles)\n",
    "        type of car\n",
    "        zip code / location\n",
    "        mileage driven per year (fuel costs, maintenance costs)\n",
    "\n",
    "\n",
    "    DONE have to pass in the tool description I think\n",
    "    \n",
    "    DONE review the output parser. may be able to repurpose some of these\n",
    "        https://langchain-ai.github.io/langgraph/tutorials/llm-compiler/LLMCompiler/#output-parser\n",
    "    DONE review the example tools. may be able to repurpose some of these\n",
    "        https://langchain-ai.github.io/langgraph/tutorials/llm-compiler/LLMCompiler/#math-tools\n",
    "\n",
    "    write the tools\n",
    "    organize tools\n",
    "\n",
    "    incorporate old prototype notes\n",
    "    DONE incorporate other notes file\n",
    "\n",
    "    read through the github code\n",
    "\n",
    "    example prompts in the input or on the interface\n",
    "\n",
    "    langsmith logging\n",
    "    \n",
    "    evaluation suite\n",
    "        what is factually correct like the federal tax prompts\n",
    "\n",
    "    examples for hte prompts\n",
    "\n",
    "    consolidate all my backlog and notes in one place when done\n",
    "\n",
    "concerns\n",
    "    lot of new stuff here for me to digest\n",
    "    will be much harder to get the minimal version up. maybe work on the smaller version simple one in parts at times too\n",
    "\n",
    "    not really sure about the maintenance and fuel tools\n",
    "        how to hook up\n",
    "        how they work\n",
    "        if the output being passed back is structured enough\n",
    "\n",
    "    think my tools are not really correctly done\n",
    "\n",
    "    not sure if I need a tool for used/new distinction or if the LLM can figure it out on toola ssignment\n",
    "\n",
    "questions\n",
    "    not getting what the messages placeholder is filled in\n",
    "    need to do a lot more research on other things that are available\n",
    "\n",
    "References\n",
    "# https://langchain-ai.github.io/langgraph/tutorials/llm-compiler/LLMCompiler/\n",
    "# https://arxiv.org/abs/2312.04511\n",
    "# https://github.com/SqueezeAILab/LLMCompiler\n",
    "\n",
    "backlog\n",
    "    additional user output instructions\n",
    "\n",
    "    user feedback for correct or incorrect info\n",
    "        for verifying rebate / maintenance info\n",
    "\n",
    "    using like ab ackground process to get info for certain things rather than query from scratch every time\n",
    "        lay out how things would be done long term if storing some results\n",
    "            background process for updates to federal data\n",
    "            background process for updates to state or local data, then cache and store in DB\n",
    "                or the first time it is done then store it\n",
    "            maybe for insurance info too\n",
    "\n",
    "    output\n",
    "        tools for output if not being formated better\n",
    "        examples for output if not being formatted better\n",
    "\n",
    "    more general EV questiosn\n",
    "\n",
    "    understand the chatprompttemplate better\n",
    "        placeholders\n",
    "        hub\n",
    "        how to better use existing prompts, write and contritube prompts\n",
    "\n",
    "    async implementation of tavily needed?\n",
    "\n",
    "    tavily alternatives\n",
    "\n",
    "    tavily with get context or more base method than the built in tool\n",
    "\n",
    "    better new and used car price tool\n",
    "\n",
    "    new and used federal tax credit tools\n",
    "        make context dynamic\n",
    "        way to make the questions more dynamic\n",
    "        is this even the best way to do it?\n",
    "\n",
    "    can probably store a lot of the data and answers\n",
    "    non US locations\n",
    "    can specifically ask for\n",
    "        kw/hour cost\n",
    "        per gallon gas price\n",
    "    more advanced reasoning graphs\n",
    "    Cost of capital and more advanced financial analysis\n",
    "\n",
    "    advanced RAG of key information\n",
    "        ie like the IRS website and how often should I updated it\n",
    "\n",
    "    tools probably not needed anymore\n",
    "        zip to state: LLM seems able to handle this\n",
    "\n",
    "    handle up to x vehicles\n",
    "    \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "# import pgeocode\n",
    "\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_pass(var: str):\n",
    "    if var not in os.environ:\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_get_pass(\"OPENAI_API_KEY\")\n",
    "_get_pass(\"LANGSMITH_API_KEY\")\n",
    "_get_pass(\"TAVILY_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"EV Prototype LLMCompiler v0.01\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_graph(graph):\n",
    "    try:\n",
    "        display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "    except Exception:\n",
    "        # This requires some extra dependencies and is optional\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid format specifier",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmath_tools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_math_tool\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mparse_user_input_tools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_parse_user_input_tool\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# from maintenance_tools import get_maintenance_tool\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# from fuel_tools import get_fuel_tool\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# from prototype.car_price_tools import get_ask_llm_tool\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# from prototype.new_car_federal_rebate_tools import get_new_federal_rebate_tool\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# from prototype.used_car_federal_rebate_tools import get_used_federal_rebate_tool\u001b[39;00m\n\u001b[0;32m     10\u001b[0m calculate \u001b[38;5;241m=\u001b[39m get_math_tool(ChatOpenAI(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4-turbo-preview\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\james\\github_repos\\untitled_09_2024_repo\\prototype\\parse_user_input_tools.py:100\u001b[0m\n\u001b[0;32m     29\u001b[0m vehicle_parse_json_schema \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$schema\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://json-schema.org/draft-07/schema#\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     31\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequired\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvehicle_list\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     91\u001b[0m     }\n\u001b[0;32m     93\u001b[0m _PARSE_USER_INPUT_TOOLS_DESCRIPTION \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparse_user_input(problem: str, context: Optional[list[str]]) -> str:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful assistant. Parse the user input to JSON format per the prompt.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour answer will not go to the user. Your answer will be used as one part of a larger question and workflow.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;66;03m# add examples?\u001b[39;00m\n\u001b[0;32m     98\u001b[0m )\n\u001b[1;32m--> 100\u001b[0m _SYSTEM_PROMPT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;124mGiven a list of user inputs, extract the vehicle information and output to a JSON format.\u001b[39m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124mIf the user has not given that information, leave the information null.\u001b[39m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124mIf the user has given multiple vehicles, have an entry per each vehicle.\u001b[39m\n\u001b[0;32m    104\u001b[0m \n\u001b[0;32m    105\u001b[0m \u001b[38;5;124mFor each vehicle, get the following fields:\u001b[39m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;124m    - purchase_type: new or used\u001b[39m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;124m    - year: the year of the vehicle (if applicable)\u001b[39m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124m    - make: the manufacturer of the vehicle\u001b[39m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;124m    - model: the model of the vehicle\u001b[39m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;124m    - price: the price of the vehicle\u001b[39m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124m    - mileage_minimum: the minimum mileage\u001b[39m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124m    - mileage_maximum: the maximum mileage\u001b[39m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124m    - miles_driven_per_year: user miles driven per year\u001b[39m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;124m    - annual_maintenance_cost: estimated annual maintenance cost\u001b[39m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;124m    - annual_insurance_cost: estimated annual insurance cost\u001b[39m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124m    \u001b[39m\n\u001b[0;32m    117\u001b[0m \n\u001b[0;32m    118\u001b[0m \u001b[38;5;124mJSON Schema: \u001b[39m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvehicle_parse_json_schema\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \n\u001b[0;32m    121\u001b[0m \u001b[38;5;124mExamples:\u001b[39m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;124m- Question: I want to know if the cost of a owning new Tesla Model Y versus a used Ford F-150 with 50,000 miles over the next five years.\u001b[39m\n\u001b[0;32m    123\u001b[0m \n\u001b[0;32m    124\u001b[0m \u001b[38;5;124m- Answer: [\u001b[39m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvehicle_class\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmid-size SUV\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: 2024,\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmake\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesla\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Y\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: null, \u001b[39m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmileage_minimum\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: null, \u001b[39m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmileage_maximum\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: null, \u001b[39m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmiles_driven_per_year\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: null, \u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mannual_maintenance_cost\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: null, \u001b[39m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mannual_insurance_cost\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: null\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mused\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvehicle_class\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlight-duty truck\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: 2021,\u001b[39m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmake\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFord\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF-150\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: null, \u001b[39m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmileage_minimum\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: 40000, \u001b[39m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmileage_maximum\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: 60000,\u001b[39m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmiles_driven_per_year\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: null, \u001b[39m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mannual_maintenance_cost\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: null, \u001b[39m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mannual_insurance_cost\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: null\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\n\u001b[0;32m    147\u001b[0m \n\u001b[0;32m    148\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    150\u001b[0m _ADDITIONAL_CONTEXT_PROMPT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_parse_user_input_tool\u001b[39m(llm: ChatOpenAI):\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid format specifier"
     ]
    }
   ],
   "source": [
    "from math_tools import get_math_tool\n",
    "from parse_user_input_tools import get_parse_user_input_tool\n",
    "\n",
    "# from maintenance_tools import get_maintenance_tool\n",
    "# from fuel_tools import get_fuel_tool\n",
    "# from prototype.car_price_tools import get_ask_llm_tool\n",
    "# from prototype.new_car_federal_rebate_tools import get_new_federal_rebate_tool\n",
    "# from prototype.used_car_federal_rebate_tools import get_used_federal_rebate_tool\n",
    "\n",
    "calculate = get_math_tool(ChatOpenAI(model=\"gpt-4-turbo-preview\"))\n",
    "parse_user_input_tool = get_parse_user_input_tool(ChatOpenAI(model=\"gpt-4-turbo-preview\"))\n",
    "# maintenance_cost_tool = get_maintenance_tool(ChatOpenAI(model=\"gpt-4-turbo-preview\"))\n",
    "# fuel_cost_tool = get_fuel_tool(ChatOpenAI(model=\"gpt-4-turbo-preview\"))\n",
    "# ask_llm_tools = get_ask_llm_tool(ChatOpenAI(model=\"gpt-4-turbo-preview\"))\n",
    "# new_federal_rebate_tool = get_new_federal_rebate_tool(ChatOpenAI(model=\"gpt-4-turbo-preview\"))\n",
    "# used_federal_rebate_tool = get_used_federal_rebate_tool(ChatOpenAI(model=\"gpt-4-turbo-preview\"))\n",
    "\n",
    "\n",
    "# search = TavilySearchResults(\n",
    "#     max_results=1,\n",
    "#     description='tavily_search_results_json(query=\"the search query\") - a search engine.',\n",
    "# )\n",
    "\n",
    "tools = [\n",
    "        calculate,\n",
    "        parse_user_input_tool,\n",
    "        # maintenance_cost_tool,\n",
    "        # fuel_cost_tool,\n",
    "        # ask_llm_new_federal_rebate_tool,\n",
    "        # used_federal_rebate_tool,\n",
    "        # ask_llm_tool,\n",
    "        #search, \n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "Given a user query, create a plan to solve it with the utmost parallelizability. Each plan should comprise an action from the following \u001b[33;1m\u001b[1;3m{num_tools}\u001b[0m types:\n",
      "\u001b[33;1m\u001b[1;3m{tool_descriptions}\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m{num_tools}\u001b[0m. join(): Collects and combines results from prior actions.\n",
      "\n",
      " - An LLM agent is called upon invoking join() to either finalize the user query or wait until the plans are executed.\n",
      " - join should always be the last action in the plan, and will be called in two scenarios:\n",
      "   (a) if the answer can be determined by gathering the outputs from tasks to generate the final response.\n",
      "   (b) if the answer cannot be determined in the planning phase before you execute the plans. Guidelines:\n",
      " - Each action described above contains input/output types and description.\n",
      "    - You must strictly adhere to the input and output types for each action.\n",
      "    - The action descriptions contain the guidelines. You MUST strictly follow those guidelines when you use the actions.\n",
      " - Each action in the plan should strictly be one of the above types. Follow the Python conventions for each action.\n",
      " - Each action MUST have a unique ID, which is strictly increasing.\n",
      " - Inputs for actions can either be constants or outputs from preceding actions. In the latter case, use the format $id to denote the ID of the previous action whose output will be the input.\n",
      " - Always call join as the last action in the plan. Say '<END_OF_PLAN>' after you call join\n",
      " - Ensure the plan maximizes parallelizability.\n",
      " - Only use the provided action types. If a query cannot be addressed using these, invoke the join action for the next steps.\n",
      " - Never introduce new actions other than the ones provided.\n",
      "\n",
      "=============================\u001b[1m Messages Placeholder \u001b[0m=============================\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{messages}\u001b[0m\n",
      "\n",
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "Remember, ONLY respond with the task list in the correct format! E.g.:\n",
      "idx. tool(arg_name=args)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from typing import Sequence\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.messages import (\n",
    "    BaseMessage,\n",
    "    FunctionMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableBranch\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from output_parser import LLMCompilerPlanParser, Task\n",
    "\n",
    "prompt = hub.pull(\"wfh/llm-compiler\")\n",
    "print(prompt.pretty_print())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_planner(\n",
    "    llm: BaseChatModel, tools: Sequence[BaseTool], base_prompt: ChatPromptTemplate\n",
    "):\n",
    "    tool_descriptions = \"\\n\".join(\n",
    "        f\"{i+1}. {tool.description}\\n\"\n",
    "        for i, tool in enumerate(\n",
    "            tools\n",
    "        )  # +1 to offset the 0 starting index, we want it count normally from 1.\n",
    "    )\n",
    "    planner_prompt = base_prompt.partial(\n",
    "        replan=\"\",\n",
    "        num_tools=len(tools)\n",
    "        + 1,  # Add one because we're adding the join() tool at the end.\n",
    "        tool_descriptions=tool_descriptions,\n",
    "    )\n",
    "    replanner_prompt = base_prompt.partial(\n",
    "        replan=' - You are given \"Previous Plan\" which is the plan that the previous agent created along with the execution results '\n",
    "        \"(given as Observation) of each plan and a general thought (given as Thought) about the executed results.\"\n",
    "        'You MUST use these information to create the next plan under \"Current Plan\".\\n'\n",
    "        ' - When starting the Current Plan, you should start with \"Thought\" that outlines the strategy for the next plan.\\n'\n",
    "        \" - In the Current Plan, you should NEVER repeat the actions that are already executed in the Previous Plan.\\n\"\n",
    "        \" - You must continue the task index from the end of the previous one. Do not repeat task indices.\",\n",
    "        num_tools=len(tools) + 1,\n",
    "        tool_descriptions=tool_descriptions,\n",
    "    )\n",
    "\n",
    "    def should_replan(state: list):\n",
    "        # Context is passed as a system message\n",
    "        return isinstance(state[-1], SystemMessage)\n",
    "\n",
    "    def wrap_messages(state: list):\n",
    "        return {\"messages\": state}\n",
    "\n",
    "    def wrap_and_get_last_index(state: list):\n",
    "        next_task = 0\n",
    "        for message in state[::-1]:\n",
    "            if isinstance(message, FunctionMessage):\n",
    "                next_task = message.additional_kwargs[\"idx\"] + 1\n",
    "                break\n",
    "        state[-1].content = state[-1].content + f\" - Begin counting at : {next_task}\"\n",
    "        return {\"messages\": state}\n",
    "\n",
    "    return (\n",
    "        RunnableBranch(\n",
    "            (should_replan, wrap_and_get_last_index | replanner_prompt),\n",
    "            wrap_messages | planner_prompt,\n",
    "        )\n",
    "        | llm\n",
    "        | LLMCompilerPlanParser(tools=tools)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4-turbo-preview\")\n",
    "# This is the primary \"agent\" in our application\n",
    "planner = create_planner(llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description='tavily_search_results_json(query=\"the search query\") - a search engine.' max_results=1 api_wrapper=TavilySearchAPIWrapper(tavily_api_key=SecretStr('**********')) {'query': 'current temperature in San Francisco'}\n",
      "---\n",
      "name='math' description='math(problem: str, context: Optional[list[str]]) -> float:\\n - Solves the provided math problem.\\n - `problem` can be either a simple math problem (e.g. \"1 + 3\") or a word problem (e.g. \"how many apples are there if there are 3 apples and 2 apples\").\\n - You cannot calculate multiple expressions in one call. For instance, `math(\\'1 + 3, 2 + 4\\')` does not work. If you need to calculate multiple expressions, you need to call them separately like `math(\\'1 + 3\\')` and then `math(\\'2 + 4\\')`\\n - Minimize the number of `math` actions as much as possible. For instance, instead of calling 2. math(\"what is the 10% of $1\") and then call 3. math(\"$1 + $2\"), you MUST call 2. math(\"what is the 110% of $1\") instead, which will reduce the number of math actions.\\n - You can optionally provide a list of strings as `context` to help the agent solve the problem. If there are multiple contexts you need to answer the question, you can provide them as a list of strings.\\n - `math` action will not see the output of the previous actions unless you provide it as `context`. You MUST provide the output of the previous actions as `context` if you need to do math on it.\\n - You MUST NEVER provide `search` type action\\'s outputs as a variable in the `problem` argument. This is because `search` returns a text blob that contains the information about the entity, not a number or value. Therefore, when you need to provide an output of `search` action, you MUST provide it as a `context` argument to `math` action. For example, 1. search(\"Barack Obama\") and then 2. math(\"age of $1\") is NEVER allowed. Use 2. math(\"age of Barack Obama\", context=[\"$1\"]) instead.\\n - When you ask a question about `context`, specify the units. For instance, \"what is xx in height?\" or \"what is xx in millions?\" instead of \"what is xx?\"' args_schema=<class 'langchain_core.utils.pydantic.math'> func=<function get_math_tool.<locals>.calculate_expression at 0x000001F14E845C10> {'problem': '$1 raised to the 3rd power', 'context': ['$1']}\n",
      "---\n",
      "join ()\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "example_question = \"What's the temperature in SF raised to the 3rd power?\"\n",
    "\n",
    "for task in planner.stream([HumanMessage(content=example_question)]):\n",
    "    print(task[\"tool\"], task[\"args\"])\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task Fetching Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, wait\n",
    "from typing import Any, Dict, Iterable, List, Union\n",
    "\n",
    "from langchain_core.runnables import (\n",
    "    chain as as_runnable,\n",
    ")\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "def _get_observations(messages: List[BaseMessage]) -> Dict[int, Any]:\n",
    "    # Get all previous tool responses\n",
    "    results = {}\n",
    "    for message in messages[::-1]:\n",
    "        if isinstance(message, FunctionMessage):\n",
    "            results[int(message.additional_kwargs[\"idx\"])] = message.content\n",
    "    return results\n",
    "\n",
    "\n",
    "class SchedulerInput(TypedDict):\n",
    "    messages: List[BaseMessage]\n",
    "    tasks: Iterable[Task]\n",
    "\n",
    "\n",
    "def _execute_task(task, observations, config):\n",
    "    tool_to_use = task[\"tool\"]\n",
    "    if isinstance(tool_to_use, str):\n",
    "        return tool_to_use\n",
    "    args = task[\"args\"]\n",
    "    try:\n",
    "        if isinstance(args, str):\n",
    "            resolved_args = _resolve_arg(args, observations)\n",
    "        elif isinstance(args, dict):\n",
    "            resolved_args = {\n",
    "                key: _resolve_arg(val, observations) for key, val in args.items()\n",
    "            }\n",
    "        else:\n",
    "            # This will likely fail\n",
    "            resolved_args = args\n",
    "    except Exception as e:\n",
    "        return (\n",
    "            f\"ERROR(Failed to call {tool_to_use.name} with args {args}.)\"\n",
    "            f\" Args could not be resolved. Error: {repr(e)}\"\n",
    "        )\n",
    "    try:\n",
    "        return tool_to_use.invoke(resolved_args, config)\n",
    "    except Exception as e:\n",
    "        return (\n",
    "            f\"ERROR(Failed to call {tool_to_use.name} with args {args}.\"\n",
    "            + f\" Args resolved to {resolved_args}. Error: {repr(e)})\"\n",
    "        )\n",
    "\n",
    "\n",
    "def _resolve_arg(arg: Union[str, Any], observations: Dict[int, Any]):\n",
    "    # $1 or ${1} -> 1\n",
    "    ID_PATTERN = r\"\\$\\{?(\\d+)\\}?\"\n",
    "\n",
    "    def replace_match(match):\n",
    "        # If the string is ${123}, match.group(0) is ${123}, and match.group(1) is 123.\n",
    "\n",
    "        # Return the match group, in this case the index, from the string. This is the index\n",
    "        # number we get back.\n",
    "        idx = int(match.group(1))\n",
    "        return str(observations.get(idx, match.group(0)))\n",
    "\n",
    "    # For dependencies on other tasks\n",
    "    if isinstance(arg, str):\n",
    "        return re.sub(ID_PATTERN, replace_match, arg)\n",
    "    elif isinstance(arg, list):\n",
    "        return [_resolve_arg(a, observations) for a in arg]\n",
    "    else:\n",
    "        return str(arg)\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "def schedule_task(task_inputs, config):\n",
    "    task: Task = task_inputs[\"task\"]\n",
    "    observations: Dict[int, Any] = task_inputs[\"observations\"]\n",
    "    try:\n",
    "        observation = _execute_task(task, observations, config)\n",
    "    except Exception:\n",
    "        import traceback\n",
    "\n",
    "        observation = traceback.format_exception()  # repr(e) +\n",
    "    observations[task[\"idx\"]] = observation\n",
    "\n",
    "\n",
    "def schedule_pending_task(\n",
    "    task: Task, observations: Dict[int, Any], retry_after: float = 0.2\n",
    "):\n",
    "    while True:\n",
    "        deps = task[\"dependencies\"]\n",
    "        if deps and (any([dep not in observations for dep in deps])):\n",
    "            # Dependencies not yet satisfied\n",
    "            time.sleep(retry_after)\n",
    "            continue\n",
    "        schedule_task.invoke({\"task\": task, \"observations\": observations})\n",
    "        break\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "def schedule_tasks(scheduler_input: SchedulerInput) -> List[FunctionMessage]:\n",
    "    \"\"\"Group the tasks into a DAG schedule.\"\"\"\n",
    "    # For streaming, we are making a few simplifying assumption:\n",
    "    # 1. The LLM does not create cyclic dependencies\n",
    "    # 2. That the LLM will not generate tasks with future deps\n",
    "    # If this ceases to be a good assumption, you can either\n",
    "    # adjust to do a proper topological sort (not-stream)\n",
    "    # or use a more complicated data structure\n",
    "    tasks = scheduler_input[\"tasks\"]\n",
    "    args_for_tasks = {}\n",
    "    messages = scheduler_input[\"messages\"]\n",
    "    # If we are re-planning, we may have calls that depend on previous\n",
    "    # plans. Start with those.\n",
    "    observations = _get_observations(messages)\n",
    "    task_names = {}\n",
    "    originals = set(observations)\n",
    "    # ^^ We assume each task inserts a different key above to\n",
    "    # avoid race conditions...\n",
    "    futures = []\n",
    "    retry_after = 0.25  # Retry every quarter second\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        for task in tasks:\n",
    "            deps = task[\"dependencies\"]\n",
    "            task_names[task[\"idx\"]] = (\n",
    "                task[\"tool\"] if isinstance(task[\"tool\"], str) else task[\"tool\"].name\n",
    "            )\n",
    "            args_for_tasks[task[\"idx\"]] = task[\"args\"]\n",
    "            if (\n",
    "                # Depends on other tasks\n",
    "                deps and (any([dep not in observations for dep in deps]))\n",
    "            ):\n",
    "                futures.append(\n",
    "                    executor.submit(\n",
    "                        schedule_pending_task, task, observations, retry_after\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                # No deps or all deps satisfied\n",
    "                # can schedule now\n",
    "                schedule_task.invoke(dict(task=task, observations=observations))\n",
    "                # futures.append(executor.submit(schedule_task.invoke dict(task=task, observations=observations)))\n",
    "\n",
    "        # All tasks have been submitted or enqueued\n",
    "        # Wait for them to complete\n",
    "        wait(futures)\n",
    "    # Convert observations to new tool messages to add to the state\n",
    "    new_observations = {\n",
    "        k: (task_names[k], args_for_tasks[k], observations[k])\n",
    "        for k in sorted(observations.keys() - originals)\n",
    "    }\n",
    "    tool_messages = [\n",
    "        FunctionMessage(\n",
    "            name=name,\n",
    "            content=str(obs),\n",
    "            additional_kwargs={\"idx\": k, \"args\": task_args},\n",
    "            tool_call_id=k,\n",
    "        )\n",
    "        for k, (name, task_args, obs) in new_observations.items()\n",
    "    ]\n",
    "    return tool_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "def plan_and_schedule(state):\n",
    "    messages = state[\"messages\"]\n",
    "    tasks = planner.stream(messages)\n",
    "    # Begin executing the planner immediately\n",
    "    try:\n",
    "        tasks = itertools.chain([next(tasks)], tasks)\n",
    "    except StopIteration:\n",
    "        # Handle the case where tasks is empty.\n",
    "        tasks = iter([])\n",
    "    scheduled_tasks = schedule_tasks.invoke(\n",
    "        {\n",
    "            \"messages\": messages,\n",
    "            \"tasks\": tasks,\n",
    "        }\n",
    "    )\n",
    "    return {\"messages\": scheduled_tasks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_joiner_prompt_system_message = \"\"\"\n",
    "================================ System Message ================================\n",
    "\n",
    "You are an assistant that helps gather information for people deciding to buy electric vehicle or a gas car. Here are some guidelines:\n",
    " - In the Assistant Scratchpad, you will be given results of a plan you have executed to answer the user's question.\n",
    " - Thought needs to reason about the question based on the Observations in 1-2 sentences.\n",
    " - Ignore irrelevant action results.\n",
    " - If the required information is present, give a concise but complete and helpful answer to the user's question.\n",
    "    - Answer in the format of:\n",
    "        - {vehicle 1}: {initial cost}. {vehicle 2} : {initial cost}. \n",
    "        - {vehicle 1}: {total cost over 5 years}. {vehicle 2}: {total cost over 5 years}.\n",
    "        - Table in the format:\n",
    "            - Year      | Vehicle 1             | Vehicle 2\n",
    "            - Year 1    | {cost after year 1}   | {cost after year 1}\n",
    "            - Year 2    | {cost after year 2}   | {cost after year 2}\n",
    "            - Year 3    | {cost after year 3}   | {cost after year 3}\n",
    "            - Year 4    | {cost after year 4}   | {cost after year 4}\n",
    "            - Year 5    | {cost after year 5}   | {cost after year 5}\n",
    "        - Total cost by expense per vehicle.\n",
    "    - If the user wants a different number of years, format, or breakdown of the costs, try to meet the user request\n",
    " - If the user did not provide this information ask them for the following pieces of information:\n",
    "    - Location\n",
    "    - Used or new\n",
    "    - If the user wants a used car, the maximum number of miles on it\n",
    "    - Type of car they are interested in (sedan, luxury, compact-SUV, mid-size truck, etc.)\n",
    "    - Type of car they are interested in (sedan, luxury, compact-SUV, mid-size truck, etc.)\n",
    "    - How many miles the user drives a year\n",
    "\n",
    " - If you are unable to give a satisfactory finishing answer, replan to get the required information. Respond in the following format:\n",
    "\n",
    "Thought: <reason about the task results and whether you have sufficient information to answer the question>\n",
    "Action: <action to take>\n",
    "Available actions:\n",
    " (1) Finish(the final answer to return to the user): returns the answer and finishes the task.\n",
    " (2) Replan(the reasoning and other information that will help you plan again. Can be a line of any length): instructs why we must replan\n",
    "\n",
    "============================= Messages Placeholder =============================\n",
    "\n",
    "{messages}\n",
    "\n",
    "================================ System Message ================================\n",
    "\n",
    "Using the above previous actions, decide whether to replan or finish. If all the required information is present. You may finish. If you have made many attempts to find the information without success, admit so and respond with whatever information you have gathered so the user can work well with you.\n",
    "\n",
    "{examples}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "ev_joiner_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(ev_joiner_prompt_system_message),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "])\n",
    "\n",
    "# # can include some examples\n",
    "# ev_joiner_prompt.partial(examples=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class FinalResponse(BaseModel):\n",
    "    \"\"\"The final response/answer.\"\"\"\n",
    "\n",
    "    response: str\n",
    "\n",
    "\n",
    "class Replan(BaseModel):\n",
    "    feedback: str = Field(\n",
    "        description=\"Analysis of the previous attempts and recommendations on what needs to be fixed.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class JoinOutputs(BaseModel):\n",
    "    \"\"\"Decide whether to replan or whether you can return the final response.\"\"\"\n",
    "\n",
    "    thought: str = Field(\n",
    "        description=\"The chain of thought reasoning for the selected action\"\n",
    "    )\n",
    "    action: Union[FinalResponse, Replan]\n",
    "\n",
    "\n",
    "joiner_prompt = hub.pull(\"wfh/llm-compiler-joiner\").partial(\n",
    "    examples=\"\"\n",
    ")  # You can optionally add examples\n",
    "llm = ChatOpenAI(model=\"gpt-4-turbo-preview\")\n",
    "\n",
    "#runnable = joiner_prompt | llm.with_structured_output(JoinOutputs)\n",
    "runnable = ev_joiner_prompt | llm.with_structured_output(JoinOutputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "Solve a question answering task. Here are some guidelines:\n",
      " - In the Assistant Scratchpad, you will be given results of a plan you have executed to answer the user's question.\n",
      " - Thought needs to reason about the question based on the Observations in 1-2 sentences.\n",
      " - Ignore irrelevant action results.\n",
      " - If the required information is present, give a concise but complete and helpful answer to the user's question.\n",
      " - If you are unable to give a satisfactory finishing answer, replan to get the required information. Respond in the following format:\n",
      "\n",
      "Thought: <reason about the task results and whether you have sufficient information to answer the question>\n",
      "Action: <action to take>\n",
      "Available actions:\n",
      " (1) Finish(the final answer to return to the user): returns the answer and finishes the task.\n",
      " (2) Replan(the reasoning and other information that will help you plan again. Can be a line of any length): instructs why we must replan\n",
      "\n",
      "=============================\u001b[1m Messages Placeholder \u001b[0m=============================\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{messages}\u001b[0m\n",
      "\n",
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "Using the above previous actions, decide whether to replan or finish. If all the required information is present. You may finish. If you have made many attempts to find the information without success, admit so and respond with whatever information you have gathered so the user can work well with you.\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{examples}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "joiner_prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['messages'], input_types={'messages': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001F131AD1940>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'examples': 'doing my example here asdfasdf'}, metadata={'lc_hub_owner': 'wfh', 'lc_hub_repo': 'llm-compiler-joiner', 'lc_hub_commit_hash': 'de0b0fcab3e90e10ee7f25c67f91890b155b48b25ed3bb1df671c7881185c7b2'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=\"Solve a question answering task. Here are some guidelines:\\n - In the Assistant Scratchpad, you will be given results of a plan you have executed to answer the user's question.\\n - Thought needs to reason about the question based on the Observations in 1-2 sentences.\\n - Ignore irrelevant action results.\\n - If the required information is present, give a concise but complete and helpful answer to the user's question.\\n - If you are unable to give a satisfactory finishing answer, replan to get the required information. Respond in the following format:\\n\\nThought: <reason about the task results and whether you have sufficient information to answer the question>\\nAction: <action to take>\\nAvailable actions:\\n (1) Finish(the final answer to return to the user): returns the answer and finishes the task.\\n (2) Replan(the reasoning and other information that will help you plan again. Can be a line of any length): instructs why we must replan\"), additional_kwargs={}), MessagesPlaceholder(variable_name='messages'), SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['examples'], input_types={}, partial_variables={}, template='Using the above previous actions, decide whether to replan or finish. If all the required information is present. You may finish. If you have made many attempts to find the information without success, admit so and respond with whatever information you have gathered so the user can work well with you.\\n\\n{examples}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.prompts.chat.ChatPromptTemplate"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_joiner_output(decision: JoinOutputs) -> List[BaseMessage]:\n",
    "    response = [AIMessage(content=f\"Thought: {decision.thought}\")]\n",
    "    if isinstance(decision.action, Replan):\n",
    "        return {\n",
    "            \"messages\": response\n",
    "            + [\n",
    "                SystemMessage(\n",
    "                    content=f\"Context from last attempt: {decision.action.feedback}\"\n",
    "                )\n",
    "            ]\n",
    "        }\n",
    "    else:\n",
    "        return {\"messages\": response + [AIMessage(content=decision.action.response)]}\n",
    "\n",
    "\n",
    "def select_recent_messages(state) -> dict:\n",
    "    messages = state[\"messages\"]\n",
    "    selected = []\n",
    "    for msg in messages[::-1]:\n",
    "        selected.append(msg)\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            break\n",
    "    return {\"messages\": selected[::-1]}\n",
    "\n",
    "\n",
    "joiner = select_recent_messages | runnable | _parse_joiner_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compose using LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# 1.  Define vertices\n",
    "# We defined plan_and_schedule above already\n",
    "# Assign each node to a state variable to update\n",
    "graph_builder.add_node(\"plan_and_schedule\", plan_and_schedule)\n",
    "graph_builder.add_node(\"join\", joiner)\n",
    "\n",
    "\n",
    "## Define edges\n",
    "graph_builder.add_edge(\"plan_and_schedule\", \"join\")\n",
    "\n",
    "### This condition determines looping logic\n",
    "\n",
    "\n",
    "def should_continue(state):\n",
    "    messages = state[\"messages\"]\n",
    "    if isinstance(messages[-1], AIMessage):\n",
    "        return END\n",
    "    return \"plan_and_schedule\"\n",
    "\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"join\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    ")\n",
    "graph_builder.add_edge(START, \"plan_and_schedule\")\n",
    "chain = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_graph(chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plan_and_schedule': {'messages': [FunctionMessage(content=\"[{'url': 'https://www.investopedia.com/articles/investing/011516/new-yorks-economy-6-industries-driving-gdp-growth.asp', 'content': 'The manufacturing sector is a leader in railroad rolling stock, as many of the earliest railroads were financed or founded in New York; garments, as New York City is the fashion capital of the U.S.; elevator parts; glass; and many other products.\\\\n Educational Services\\\\nThough not typically thought of as a leading industry, the educational sector in New York nonetheless has a substantial impact on the state and its residents, and in attracting new talent that eventually enters the New York business scene. New York has seen a large uptick in college attendees, both young and old, over the 21st century, and an increasing number of new employees in other New York sectors were educated in the state. New York City is the leading job hub for banking, finance, and communication in the U.S. New York is also a major manufacturing center and shipping port, and it has a thriving technological sector.\\\\n The state of New York has the third-largest economy in the United States with a gross domestic product (GDP) of $1.7 trillion, trailing only Texas and California.'}]\", additional_kwargs={'idx': 1, 'args': {'query': 'GDP of New York'}}, response_metadata={}, name='tavily_search_results_json', id='04ce0098-d613-4e4c-bae9-e13402256afa', tool_call_id=1), FunctionMessage(content='join', additional_kwargs={'idx': 2, 'args': ()}, response_metadata={}, name='join', id='58b4770a-91f6-4b1e-b7d0-303f14e3a280', tool_call_id=2)]}}\n",
      "---\n",
      "{'join': {'messages': [AIMessage(content=\"Thought: The information provided directly answers the user's question, giving the GDP of New York.\", additional_kwargs={}, response_metadata={}, id='d3c509dc-4f72-4fd6-86e0-4cd200286965'), AIMessage(content='The GDP of New York is $1.7 trillion.', additional_kwargs={}, response_metadata={}, id='129683ae-d69d-43ca-b63d-42f9c5feebbb')]}}\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for step in chain.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"What's the GDP of New York?\")]}\n",
    "):\n",
    "    print(step)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The GDP of New York is $1.7 trillion.\n"
     ]
    }
   ],
   "source": [
    "# Final answer\n",
    "print(step[\"join\"][\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the math tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plan_and_schedule': {'messages': [FunctionMessage(content='1440', additional_kwargs={'idx': 1, 'args': {'problem': '120 * 12'}}, response_metadata={}, name='math', id='867dbf43-1505-4daf-ae0e-b45aed598dae', tool_call_id=1)]}}\n",
      "---\n",
      "{'join': {'messages': [AIMessage(content=\"Thought: The calculation to find the yearly cost of car insurance is straightforward: $120/month * 12 months/year = $1440/year. This directly answers the user's question.\", additional_kwargs={}, response_metadata={}, id='cf14cb9b-b344-4db2-bf15-ec4a7ae04fb8'), AIMessage(content='The car insurance costs $1440 per year if it is $120 per month.', additional_kwargs={}, response_metadata={}, id='42c58b73-f06b-40a2-a2f2-dc8f8b9c0fc7')]}}\n",
      "---\n",
      "The car insurance costs $1440 per year if it is $120 per month.\n"
     ]
    }
   ],
   "source": [
    "for step in chain.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"If the car insurance is $120 per month, how much is it per year?\")]}\n",
    "):\n",
    "    print(step)\n",
    "    print(\"---\")\n",
    "\n",
    "print(step[\"join\"][\"messages\"][-1].content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sep2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
