{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nworking\\n    \\n    get some version of it working\\n        STOPPED AT PLANNER\\n        may have to revert back to full tutorial code first to see\\n    DONE read through the tutorial\\n        rough outline\\n    DONE read through the paper\\n    DONE see the other tutorial examples in experimental\\n    \\n\\nto do\\n    focus on buy only\\n        simple things first, make notes for where more improvements\\n        new / used\\n            by model and num miles\\n        federal tax credit only\\n        costs\\n            maintenance\\n            insurance\\n            registration\\n            fuel\\n\\n    after initial question, ask for more details with an example\\n        new/used (and how many miles)\\n        type of car\\n        zip code / location\\n\\n\\n    have to pass in the tool description I think\\n    \\n    review the output parser. may be able to repurpose some of these\\n        https://langchain-ai.github.io/langgraph/tutorials/llm-compiler/LLMCompiler/#output-parser\\n    review the example tools. may be able to repurpose some of these\\n        https://langchain-ai.github.io/langgraph/tutorials/llm-compiler/LLMCompiler/#math-tools\\n\\n    write the tools\\n    organize tools\\n\\n    incorporate old prototype notes\\n    incorporate other notes file\\n\\n    read through the github code\\n\\nconcerns\\n    lot of new stuff here for me to digest\\n    will be much harder to get the minimal version up. maybe work on the smaller version simple one in parts at times too\\n\\nquestions\\n    not getting what the messages placeholder is filled in\\n\\nReferences\\n# https://langchain-ai.github.io/langgraph/tutorials/llm-compiler/LLMCompiler/\\n# https://arxiv.org/abs/2312.04511\\n# https://github.com/SqueezeAILab/LLMCompiler\\n\\nbacklog\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "working\n",
    "    DONE update the entry prompt\n",
    "    \n",
    "    provide examples\n",
    "        overall question\n",
    "        fuel tools\n",
    "        maintenance tools\n",
    "\n",
    "    work on the output final look\n",
    "        as this will influence how the tools work\n",
    "        have it mocked up but how do I connect it to the below pipeline?\n",
    "        research and test some output examples. should come out of hte box for LLM\n",
    "        feed in some examples\n",
    "\n",
    "    create tools for my simple value version\n",
    "        cars available\n",
    "            use google or main website? how does tavily do for shopping\n",
    "        federal tax credit info\n",
    "            can tavily search a specific website\n",
    "        TEST average maintenance costs\n",
    "        TEST fuel and gas costs\n",
    "        DONE LLM turns a monthly cost into a yearly cost\n",
    "            adds the yearly costs\n",
    "            can use existing math tools\n",
    "            have to find a way to format the question correctly\n",
    "            then get the output in a format that works\n",
    "                ask the LLM or give a specific structured output?\n",
    "        TRY WITH LLM FIRST formats the outputs correctly\n",
    "\n",
    "\n",
    "    DONE get some version of it working\n",
    "        STOPPED AT PLANNER\n",
    "        may have to revert back to full tutorial code first to see\n",
    "    DONE read through the tutorial\n",
    "        rough outline\n",
    "    DONE read through the paper\n",
    "    DONE see the other tutorial examples in experimental\n",
    "    \n",
    "\n",
    "to do\n",
    "    focus on buy only\n",
    "        simple things first, make notes for where more improvements\n",
    "        new / used\n",
    "            by model and num miles\n",
    "        federal tax credit only\n",
    "        costs\n",
    "            maintenance\n",
    "                does this depend on miles driven per yer? startw ith default, let user change\n",
    "            insurance\n",
    "            registration\n",
    "            fuel\n",
    "\n",
    "    after initial question, ask for more details with an example\n",
    "        new/used (and how many miles)\n",
    "        type of car\n",
    "        zip code / location\n",
    "        mileage driven per year (fuel costs, maintenance costs)\n",
    "\n",
    "\n",
    "    have to pass in the tool description I think\n",
    "    \n",
    "    review the output parser. may be able to repurpose some of these\n",
    "        https://langchain-ai.github.io/langgraph/tutorials/llm-compiler/LLMCompiler/#output-parser\n",
    "    review the example tools. may be able to repurpose some of these\n",
    "        https://langchain-ai.github.io/langgraph/tutorials/llm-compiler/LLMCompiler/#math-tools\n",
    "\n",
    "    write the tools\n",
    "    organize tools\n",
    "\n",
    "    incorporate old prototype notes\n",
    "    incorporate other notes file\n",
    "\n",
    "    read through the github code\n",
    "\n",
    "    example prompts in the input or on the interface\n",
    "\n",
    "    langsmith logging\n",
    "    \n",
    "    evaluation suite\n",
    "\n",
    "concerns\n",
    "    lot of new stuff here for me to digest\n",
    "    will be much harder to get the minimal version up. maybe work on the smaller version simple one in parts at times too\n",
    "\n",
    "    not really sure about the maintenance and fuel tools\n",
    "        how to hook up\n",
    "        how they work\n",
    "        if the output being passed back is structured enough\n",
    "\n",
    "questions\n",
    "    not getting what the messages placeholder is filled in\n",
    "    need to do a lot more research on other things that are available\n",
    "\n",
    "References\n",
    "# https://langchain-ai.github.io/langgraph/tutorials/llm-compiler/LLMCompiler/\n",
    "# https://arxiv.org/abs/2312.04511\n",
    "# https://github.com/SqueezeAILab/LLMCompiler\n",
    "\n",
    "backlog\n",
    "    user feedback for correct or incorrect info\n",
    "        for verifying rebate / maintenance info\n",
    "\n",
    "    using like ab ackground process to get info for certain things rather than query from scratch every time\n",
    "        lay out how things would be done long term if storing some results\n",
    "            background process for updates to federal data\n",
    "            background process for updates to state or local data, then cache and store in DB\n",
    "                or the first time it is done then store it\n",
    "            maybe for insurance info too\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "import pgeocode\n",
    "\n",
    "\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_pass(var: str):\n",
    "    if var not in os.environ:\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_get_pass(\"OPENAI_API_KEY\")\n",
    "_get_pass(\"LANGSMITH_API_KEY\")\n",
    "_get_pass(\"TAVILY_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"EV Prototype LLMCompiler v0.01\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_graph(graph):\n",
    "    try:\n",
    "        display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "    except Exception:\n",
    "        # This requires some extra dependencies and is optional\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tool\n",
    "# def get_state_and_city_from_zip_code(zip_code: str):\n",
    "#     \"\"\"Get the United States state and city from a five digit zip code\n",
    "    \n",
    "#     Returns:\n",
    "#         state_name: str: The state name. 'unknown' if the zip code is invalid\n",
    "#         place_name: str: The city name. 'unknown' if the zip code is invalid\n",
    "#     \"\"\"\n",
    "#     unknown_value = 'unknown'\n",
    "\n",
    "#     if zip_code is None or type(zip_code) is not str or len(zip_code) != 5:\n",
    "#         state_name = unknown_value\n",
    "#         place_name = unknown_value\n",
    "\n",
    "#     else:\n",
    "#         nomi = pgeocode.Nominatim('us')\n",
    "\n",
    "#         results_df = nomi.query_postal_code(zip_code)\n",
    "\n",
    "#         if results_df.shape[0] > 0:\n",
    "#             state_name = results_df.state_name\n",
    "#             place_name = results_df.place_name\n",
    "#         else:\n",
    "#             state_name = unknown_value\n",
    "#             place_name = unknown_value\n",
    "\n",
    "#     return state_name, place_name\n",
    "\n",
    "# search = TavilySearchResults(\n",
    "#     max_results=1,\n",
    "#     description='tavily_search_results_json(query=\"the search query\") - a search engine.',\n",
    "# )\n",
    "\n",
    "# test_tool_list = [\n",
    "#     search,\n",
    "#     get_state_and_city_from_zip_code,\n",
    "\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_tools import get_math_tool\n",
    "\n",
    "calculate = get_math_tool(ChatOpenAI(model=\"gpt-4-turbo-preview\"))\n",
    "search = TavilySearchResults(\n",
    "    max_results=1,\n",
    "    description='tavily_search_results_json(query=\"the search query\") - a search engine.',\n",
    ")\n",
    "\n",
    "tools = [search, calculate]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "Given a user query, create a plan to solve it with the utmost parallelizability. Each plan should comprise an action from the following \u001b[33;1m\u001b[1;3m{num_tools}\u001b[0m types:\n",
      "\u001b[33;1m\u001b[1;3m{tool_descriptions}\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m{num_tools}\u001b[0m. join(): Collects and combines results from prior actions.\n",
      "\n",
      " - An LLM agent is called upon invoking join() to either finalize the user query or wait until the plans are executed.\n",
      " - join should always be the last action in the plan, and will be called in two scenarios:\n",
      "   (a) if the answer can be determined by gathering the outputs from tasks to generate the final response.\n",
      "   (b) if the answer cannot be determined in the planning phase before you execute the plans. Guidelines:\n",
      " - Each action described above contains input/output types and description.\n",
      "    - You must strictly adhere to the input and output types for each action.\n",
      "    - The action descriptions contain the guidelines. You MUST strictly follow those guidelines when you use the actions.\n",
      " - Each action in the plan should strictly be one of the above types. Follow the Python conventions for each action.\n",
      " - Each action MUST have a unique ID, which is strictly increasing.\n",
      " - Inputs for actions can either be constants or outputs from preceding actions. In the latter case, use the format $id to denote the ID of the previous action whose output will be the input.\n",
      " - Always call join as the last action in the plan. Say '<END_OF_PLAN>' after you call join\n",
      " - Ensure the plan maximizes parallelizability.\n",
      " - Only use the provided action types. If a query cannot be addressed using these, invoke the join action for the next steps.\n",
      " - Never introduce new actions other than the ones provided.\n",
      "\n",
      "=============================\u001b[1m Messages Placeholder \u001b[0m=============================\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{messages}\u001b[0m\n",
      "\n",
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "Remember, ONLY respond with the task list in the correct format! E.g.:\n",
      "idx. tool(arg_name=args)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from typing import Sequence\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.messages import (\n",
    "    BaseMessage,\n",
    "    FunctionMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableBranch\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from output_parser import LLMCompilerPlanParser, Task\n",
    "\n",
    "prompt = hub.pull(\"wfh/llm-compiler\")\n",
    "print(prompt.pretty_print())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_planner(\n",
    "    llm: BaseChatModel, tools: Sequence[BaseTool], base_prompt: ChatPromptTemplate\n",
    "):\n",
    "    tool_descriptions = \"\\n\".join(\n",
    "        f\"{i+1}. {tool.description}\\n\"\n",
    "        for i, tool in enumerate(\n",
    "            tools\n",
    "        )  # +1 to offset the 0 starting index, we want it count normally from 1.\n",
    "    )\n",
    "    planner_prompt = base_prompt.partial(\n",
    "        replan=\"\",\n",
    "        num_tools=len(tools)\n",
    "        + 1,  # Add one because we're adding the join() tool at the end.\n",
    "        tool_descriptions=tool_descriptions,\n",
    "    )\n",
    "    replanner_prompt = base_prompt.partial(\n",
    "        replan=' - You are given \"Previous Plan\" which is the plan that the previous agent created along with the execution results '\n",
    "        \"(given as Observation) of each plan and a general thought (given as Thought) about the executed results.\"\n",
    "        'You MUST use these information to create the next plan under \"Current Plan\".\\n'\n",
    "        ' - When starting the Current Plan, you should start with \"Thought\" that outlines the strategy for the next plan.\\n'\n",
    "        \" - In the Current Plan, you should NEVER repeat the actions that are already executed in the Previous Plan.\\n\"\n",
    "        \" - You must continue the task index from the end of the previous one. Do not repeat task indices.\",\n",
    "        num_tools=len(tools) + 1,\n",
    "        tool_descriptions=tool_descriptions,\n",
    "    )\n",
    "\n",
    "    def should_replan(state: list):\n",
    "        # Context is passed as a system message\n",
    "        return isinstance(state[-1], SystemMessage)\n",
    "\n",
    "    def wrap_messages(state: list):\n",
    "        return {\"messages\": state}\n",
    "\n",
    "    def wrap_and_get_last_index(state: list):\n",
    "        next_task = 0\n",
    "        for message in state[::-1]:\n",
    "            if isinstance(message, FunctionMessage):\n",
    "                next_task = message.additional_kwargs[\"idx\"] + 1\n",
    "                break\n",
    "        state[-1].content = state[-1].content + f\" - Begin counting at : {next_task}\"\n",
    "        return {\"messages\": state}\n",
    "\n",
    "    return (\n",
    "        RunnableBranch(\n",
    "            (should_replan, wrap_and_get_last_index | replanner_prompt),\n",
    "            wrap_messages | planner_prompt,\n",
    "        )\n",
    "        | llm\n",
    "        | LLMCompilerPlanParser(tools=tools)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4-turbo-preview\")\n",
    "# This is the primary \"agent\" in our application\n",
    "planner = create_planner(llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description='tavily_search_results_json(query=\"the search query\") - a search engine.' max_results=1 api_wrapper=TavilySearchAPIWrapper(tavily_api_key=SecretStr('**********')) {'query': 'current temperature in San Francisco'}\n",
      "---\n",
      "name='math' description='math(problem: str, context: Optional[list[str]]) -> float:\\n - Solves the provided math problem.\\n - `problem` can be either a simple math problem (e.g. \"1 + 3\") or a word problem (e.g. \"how many apples are there if there are 3 apples and 2 apples\").\\n - You cannot calculate multiple expressions in one call. For instance, `math(\\'1 + 3, 2 + 4\\')` does not work. If you need to calculate multiple expressions, you need to call them separately like `math(\\'1 + 3\\')` and then `math(\\'2 + 4\\')`\\n - Minimize the number of `math` actions as much as possible. For instance, instead of calling 2. math(\"what is the 10% of $1\") and then call 3. math(\"$1 + $2\"), you MUST call 2. math(\"what is the 110% of $1\") instead, which will reduce the number of math actions.\\n - You can optionally provide a list of strings as `context` to help the agent solve the problem. If there are multiple contexts you need to answer the question, you can provide them as a list of strings.\\n - `math` action will not see the output of the previous actions unless you provide it as `context`. You MUST provide the output of the previous actions as `context` if you need to do math on it.\\n - You MUST NEVER provide `search` type action\\'s outputs as a variable in the `problem` argument. This is because `search` returns a text blob that contains the information about the entity, not a number or value. Therefore, when you need to provide an output of `search` action, you MUST provide it as a `context` argument to `math` action. For example, 1. search(\"Barack Obama\") and then 2. math(\"age of $1\") is NEVER allowed. Use 2. math(\"age of Barack Obama\", context=[\"$1\"]) instead.\\n - When you ask a question about `context`, specify the units. For instance, \"what is xx in height?\" or \"what is xx in millions?\" instead of \"what is xx?\"' args_schema=<class 'langchain_core.utils.pydantic.math'> func=<function get_math_tool.<locals>.calculate_expression at 0x000001FF269D7AF0> {'problem': 'x ** 3', 'context': ['$1']}\n",
      "---\n",
      "join ()\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "example_question = \"What's the temperature in SF raised to the 3rd power?\"\n",
    "\n",
    "for task in planner.stream([HumanMessage(content=example_question)]):\n",
    "    print(task[\"tool\"], task[\"args\"])\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task Fetching Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, wait\n",
    "from typing import Any, Dict, Iterable, List, Union\n",
    "\n",
    "from langchain_core.runnables import (\n",
    "    chain as as_runnable,\n",
    ")\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "def _get_observations(messages: List[BaseMessage]) -> Dict[int, Any]:\n",
    "    # Get all previous tool responses\n",
    "    results = {}\n",
    "    for message in messages[::-1]:\n",
    "        if isinstance(message, FunctionMessage):\n",
    "            results[int(message.additional_kwargs[\"idx\"])] = message.content\n",
    "    return results\n",
    "\n",
    "\n",
    "class SchedulerInput(TypedDict):\n",
    "    messages: List[BaseMessage]\n",
    "    tasks: Iterable[Task]\n",
    "\n",
    "\n",
    "def _execute_task(task, observations, config):\n",
    "    tool_to_use = task[\"tool\"]\n",
    "    if isinstance(tool_to_use, str):\n",
    "        return tool_to_use\n",
    "    args = task[\"args\"]\n",
    "    try:\n",
    "        if isinstance(args, str):\n",
    "            resolved_args = _resolve_arg(args, observations)\n",
    "        elif isinstance(args, dict):\n",
    "            resolved_args = {\n",
    "                key: _resolve_arg(val, observations) for key, val in args.items()\n",
    "            }\n",
    "        else:\n",
    "            # This will likely fail\n",
    "            resolved_args = args\n",
    "    except Exception as e:\n",
    "        return (\n",
    "            f\"ERROR(Failed to call {tool_to_use.name} with args {args}.)\"\n",
    "            f\" Args could not be resolved. Error: {repr(e)}\"\n",
    "        )\n",
    "    try:\n",
    "        return tool_to_use.invoke(resolved_args, config)\n",
    "    except Exception as e:\n",
    "        return (\n",
    "            f\"ERROR(Failed to call {tool_to_use.name} with args {args}.\"\n",
    "            + f\" Args resolved to {resolved_args}. Error: {repr(e)})\"\n",
    "        )\n",
    "\n",
    "\n",
    "def _resolve_arg(arg: Union[str, Any], observations: Dict[int, Any]):\n",
    "    # $1 or ${1} -> 1\n",
    "    ID_PATTERN = r\"\\$\\{?(\\d+)\\}?\"\n",
    "\n",
    "    def replace_match(match):\n",
    "        # If the string is ${123}, match.group(0) is ${123}, and match.group(1) is 123.\n",
    "\n",
    "        # Return the match group, in this case the index, from the string. This is the index\n",
    "        # number we get back.\n",
    "        idx = int(match.group(1))\n",
    "        return str(observations.get(idx, match.group(0)))\n",
    "\n",
    "    # For dependencies on other tasks\n",
    "    if isinstance(arg, str):\n",
    "        return re.sub(ID_PATTERN, replace_match, arg)\n",
    "    elif isinstance(arg, list):\n",
    "        return [_resolve_arg(a, observations) for a in arg]\n",
    "    else:\n",
    "        return str(arg)\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "def schedule_task(task_inputs, config):\n",
    "    task: Task = task_inputs[\"task\"]\n",
    "    observations: Dict[int, Any] = task_inputs[\"observations\"]\n",
    "    try:\n",
    "        observation = _execute_task(task, observations, config)\n",
    "    except Exception:\n",
    "        import traceback\n",
    "\n",
    "        observation = traceback.format_exception()  # repr(e) +\n",
    "    observations[task[\"idx\"]] = observation\n",
    "\n",
    "\n",
    "def schedule_pending_task(\n",
    "    task: Task, observations: Dict[int, Any], retry_after: float = 0.2\n",
    "):\n",
    "    while True:\n",
    "        deps = task[\"dependencies\"]\n",
    "        if deps and (any([dep not in observations for dep in deps])):\n",
    "            # Dependencies not yet satisfied\n",
    "            time.sleep(retry_after)\n",
    "            continue\n",
    "        schedule_task.invoke({\"task\": task, \"observations\": observations})\n",
    "        break\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "def schedule_tasks(scheduler_input: SchedulerInput) -> List[FunctionMessage]:\n",
    "    \"\"\"Group the tasks into a DAG schedule.\"\"\"\n",
    "    # For streaming, we are making a few simplifying assumption:\n",
    "    # 1. The LLM does not create cyclic dependencies\n",
    "    # 2. That the LLM will not generate tasks with future deps\n",
    "    # If this ceases to be a good assumption, you can either\n",
    "    # adjust to do a proper topological sort (not-stream)\n",
    "    # or use a more complicated data structure\n",
    "    tasks = scheduler_input[\"tasks\"]\n",
    "    args_for_tasks = {}\n",
    "    messages = scheduler_input[\"messages\"]\n",
    "    # If we are re-planning, we may have calls that depend on previous\n",
    "    # plans. Start with those.\n",
    "    observations = _get_observations(messages)\n",
    "    task_names = {}\n",
    "    originals = set(observations)\n",
    "    # ^^ We assume each task inserts a different key above to\n",
    "    # avoid race conditions...\n",
    "    futures = []\n",
    "    retry_after = 0.25  # Retry every quarter second\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        for task in tasks:\n",
    "            deps = task[\"dependencies\"]\n",
    "            task_names[task[\"idx\"]] = (\n",
    "                task[\"tool\"] if isinstance(task[\"tool\"], str) else task[\"tool\"].name\n",
    "            )\n",
    "            args_for_tasks[task[\"idx\"]] = task[\"args\"]\n",
    "            if (\n",
    "                # Depends on other tasks\n",
    "                deps and (any([dep not in observations for dep in deps]))\n",
    "            ):\n",
    "                futures.append(\n",
    "                    executor.submit(\n",
    "                        schedule_pending_task, task, observations, retry_after\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                # No deps or all deps satisfied\n",
    "                # can schedule now\n",
    "                schedule_task.invoke(dict(task=task, observations=observations))\n",
    "                # futures.append(executor.submit(schedule_task.invoke dict(task=task, observations=observations)))\n",
    "\n",
    "        # All tasks have been submitted or enqueued\n",
    "        # Wait for them to complete\n",
    "        wait(futures)\n",
    "    # Convert observations to new tool messages to add to the state\n",
    "    new_observations = {\n",
    "        k: (task_names[k], args_for_tasks[k], observations[k])\n",
    "        for k in sorted(observations.keys() - originals)\n",
    "    }\n",
    "    tool_messages = [\n",
    "        FunctionMessage(\n",
    "            name=name,\n",
    "            content=str(obs),\n",
    "            additional_kwargs={\"idx\": k, \"args\": task_args},\n",
    "            tool_call_id=k,\n",
    "        )\n",
    "        for k, (name, task_args, obs) in new_observations.items()\n",
    "    ]\n",
    "    return tool_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "def plan_and_schedule(state):\n",
    "    messages = state[\"messages\"]\n",
    "    tasks = planner.stream(messages)\n",
    "    # Begin executing the planner immediately\n",
    "    try:\n",
    "        tasks = itertools.chain([next(tasks)], tasks)\n",
    "    except StopIteration:\n",
    "        # Handle the case where tasks is empty.\n",
    "        tasks = iter([])\n",
    "    scheduled_tasks = schedule_tasks.invoke(\n",
    "        {\n",
    "            \"messages\": messages,\n",
    "            \"tasks\": tasks,\n",
    "        }\n",
    "    )\n",
    "    return {\"messages\": scheduled_tasks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class FinalResponse(BaseModel):\n",
    "    \"\"\"The final response/answer.\"\"\"\n",
    "\n",
    "    response: str\n",
    "\n",
    "\n",
    "class Replan(BaseModel):\n",
    "    feedback: str = Field(\n",
    "        description=\"Analysis of the previous attempts and recommendations on what needs to be fixed.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class JoinOutputs(BaseModel):\n",
    "    \"\"\"Decide whether to replan or whether you can return the final response.\"\"\"\n",
    "\n",
    "    thought: str = Field(\n",
    "        description=\"The chain of thought reasoning for the selected action\"\n",
    "    )\n",
    "    action: Union[FinalResponse, Replan]\n",
    "\n",
    "\n",
    "joiner_prompt = hub.pull(\"wfh/llm-compiler-joiner\").partial(\n",
    "    examples=\"\"\n",
    ")  # You can optionally add examples\n",
    "llm = ChatOpenAI(model=\"gpt-4-turbo-preview\")\n",
    "\n",
    "runnable = joiner_prompt | llm.with_structured_output(JoinOutputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_joiner_output(decision: JoinOutputs) -> List[BaseMessage]:\n",
    "    response = [AIMessage(content=f\"Thought: {decision.thought}\")]\n",
    "    if isinstance(decision.action, Replan):\n",
    "        return {\n",
    "            \"messages\": response\n",
    "            + [\n",
    "                SystemMessage(\n",
    "                    content=f\"Context from last attempt: {decision.action.feedback}\"\n",
    "                )\n",
    "            ]\n",
    "        }\n",
    "    else:\n",
    "        return {\"messages\": response + [AIMessage(content=decision.action.response)]}\n",
    "\n",
    "\n",
    "def select_recent_messages(state) -> dict:\n",
    "    messages = state[\"messages\"]\n",
    "    selected = []\n",
    "    for msg in messages[::-1]:\n",
    "        selected.append(msg)\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            break\n",
    "    return {\"messages\": selected[::-1]}\n",
    "\n",
    "\n",
    "joiner = select_recent_messages | runnable | _parse_joiner_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compose using LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# 1.  Define vertices\n",
    "# We defined plan_and_schedule above already\n",
    "# Assign each node to a state variable to update\n",
    "graph_builder.add_node(\"plan_and_schedule\", plan_and_schedule)\n",
    "graph_builder.add_node(\"join\", joiner)\n",
    "\n",
    "\n",
    "## Define edges\n",
    "graph_builder.add_edge(\"plan_and_schedule\", \"join\")\n",
    "\n",
    "### This condition determines looping logic\n",
    "\n",
    "\n",
    "def should_continue(state):\n",
    "    messages = state[\"messages\"]\n",
    "    if isinstance(messages[-1], AIMessage):\n",
    "        return END\n",
    "    return \"plan_and_schedule\"\n",
    "\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"join\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    ")\n",
    "graph_builder.add_edge(START, \"plan_and_schedule\")\n",
    "chain = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_graph(chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plan_and_schedule': {'messages': [FunctionMessage(content=\"[{'url': 'https://www.investopedia.com/articles/investing/011516/new-yorks-economy-6-industries-driving-gdp-growth.asp', 'content': 'The manufacturing sector is a leader in railroad rolling stock, as many of the earliest railroads were financed or founded in New York; garments, as New York City is the fashion capital of the U.S.; elevator parts; glass; and many other products.\\\\n Educational Services\\\\nThough not typically thought of as a leading industry, the educational sector in New York nonetheless has a substantial impact on the state and its residents, and in attracting new talent that eventually enters the New York business scene. New York has seen a large uptick in college attendees, both young and old, over the 21st century, and an increasing number of new employees in other New York sectors were educated in the state. New York City is the leading job hub for banking, finance, and communication in the U.S. New York is also a major manufacturing center and shipping port, and it has a thriving technological sector.\\\\n The state of New York has the third-largest economy in the United States with a gross domestic product (GDP) of $1.7 trillion, trailing only Texas and California.'}]\", additional_kwargs={'idx': 1, 'args': {'query': 'GDP of New York'}}, response_metadata={}, name='tavily_search_results_json', id='04ce0098-d613-4e4c-bae9-e13402256afa', tool_call_id=1), FunctionMessage(content='join', additional_kwargs={'idx': 2, 'args': ()}, response_metadata={}, name='join', id='58b4770a-91f6-4b1e-b7d0-303f14e3a280', tool_call_id=2)]}}\n",
      "---\n",
      "{'join': {'messages': [AIMessage(content=\"Thought: The information provided directly answers the user's question, giving the GDP of New York.\", additional_kwargs={}, response_metadata={}, id='d3c509dc-4f72-4fd6-86e0-4cd200286965'), AIMessage(content='The GDP of New York is $1.7 trillion.', additional_kwargs={}, response_metadata={}, id='129683ae-d69d-43ca-b63d-42f9c5feebbb')]}}\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for step in chain.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"What's the GDP of New York?\")]}\n",
    "):\n",
    "    print(step)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The GDP of New York is $1.7 trillion.\n"
     ]
    }
   ],
   "source": [
    "# Final answer\n",
    "print(step[\"join\"][\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the math tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plan_and_schedule': {'messages': [FunctionMessage(content='1440', additional_kwargs={'idx': 1, 'args': {'problem': '120 * 12'}}, response_metadata={}, name='math', id='867dbf43-1505-4daf-ae0e-b45aed598dae', tool_call_id=1)]}}\n",
      "---\n",
      "{'join': {'messages': [AIMessage(content=\"Thought: The calculation to find the yearly cost of car insurance is straightforward: $120/month * 12 months/year = $1440/year. This directly answers the user's question.\", additional_kwargs={}, response_metadata={}, id='cf14cb9b-b344-4db2-bf15-ec4a7ae04fb8'), AIMessage(content='The car insurance costs $1440 per year if it is $120 per month.', additional_kwargs={}, response_metadata={}, id='42c58b73-f06b-40a2-a2f2-dc8f8b9c0fc7')]}}\n",
      "---\n",
      "The car insurance costs $1440 per year if it is $120 per month.\n"
     ]
    }
   ],
   "source": [
    "for step in chain.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"If the car insurance is $120 per month, how much is it per year?\")]}\n",
    "):\n",
    "    print(step)\n",
    "    print(\"---\")\n",
    "\n",
    "print(step[\"join\"][\"messages\"][-1].content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sep2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
